{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-12T14:32:46.227137Z",
     "iopub.status.busy": "2025-09-12T14:32:46.226894Z",
     "iopub.status.idle": "2025-09-12T14:32:46.490867Z",
     "shell.execute_reply": "2025-09-12T14:32:46.490261Z",
     "shell.execute_reply.started": "2025-09-12T14:32:46.227117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import os\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### open and read the shakespeare file in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:32:46.492472Z",
     "iopub.status.busy": "2025-09-12T14:32:46.492101Z",
     "iopub.status.idle": "2025-09-12T14:32:46.635773Z",
     "shell.execute_reply": "2025-09-12T14:32:46.635236Z",
     "shell.execute_reply.started": "2025-09-12T14:32:46.492452Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "txt = []\n",
    "directory = \"/kaggle/input/shakespeare-txt\"\n",
    "sentence = []\n",
    "for file in os.listdir(directory):\n",
    "    with open(os.path.join(directory , file) , \"r\") as f:\n",
    "        txt = f.read().lower()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:32:46.636496Z",
     "iopub.status.busy": "2025-09-12T14:32:46.636329Z",
     "iopub.status.idle": "2025-09-12T14:32:46.641744Z",
     "shell.execute_reply": "2025-09-12T14:32:46.641122Z",
     "shell.execute_reply.started": "2025-09-12T14:32:46.636481Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5436475"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### take only 60000 caracters to not face ram problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:32:46.642958Z",
     "iopub.status.busy": "2025-09-12T14:32:46.642448Z",
     "iopub.status.idle": "2025-09-12T14:32:46.652311Z",
     "shell.execute_reply": "2025-09-12T14:32:46.651545Z",
     "shell.execute_reply.started": "2025-09-12T14:32:46.642939Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = txt[0:60000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:32:46.654285Z",
     "iopub.status.busy": "2025-09-12T14:32:46.654100Z",
     "iopub.status.idle": "2025-09-12T14:32:59.580423Z",
     "shell.execute_reply": "2025-09-12T14:32:59.579845Z",
     "shell.execute_reply.started": "2025-09-12T14:32:46.654269Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 14:32:47.995663: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757687568.171856      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757687568.217079      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fit the tokenizer on the data in a word level tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:32:59.581598Z",
     "iopub.status.busy": "2025-09-12T14:32:59.581160Z",
     "iopub.status.idle": "2025-09-12T14:32:59.592433Z",
     "shell.execute_reply": "2025-09-12T14:32:59.591156Z",
     "shell.execute_reply.started": "2025-09-12T14:32:59.581578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "words_vocab = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we have 2361 unique words in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:32:59.593923Z",
     "iopub.status.busy": "2025-09-12T14:32:59.593584Z",
     "iopub.status.idle": "2025-09-12T14:32:59.618686Z",
     "shell.execute_reply": "2025-09-12T14:32:59.617924Z",
     "shell.execute_reply.started": "2025-09-12T14:32:59.593893Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2361\n"
     ]
    }
   ],
   "source": [
    "print(words_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply the tokenizer on the data so we end up with an array of indexes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:32:59.619708Z",
     "iopub.status.busy": "2025-09-12T14:32:59.619484Z",
     "iopub.status.idle": "2025-09-12T14:32:59.632764Z",
     "shell.execute_reply": "2025-09-12T14:32:59.631987Z",
     "shell.execute_reply.started": "2025-09-12T14:32:59.619691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences([data])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### so we have 10373 word and 10373 indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:32:59.633869Z",
     "iopub.status.busy": "2025-09-12T14:32:59.633553Z",
     "iopub.status.idle": "2025-09-12T14:32:59.645204Z",
     "shell.execute_reply": "2025-09-12T14:32:59.644619Z",
     "shell.execute_reply.started": "2025-09-12T14:32:59.633850Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10373"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the second most frequent word in the dataset we tokenized is and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:32:59.646438Z",
     "iopub.status.busy": "2025-09-12T14:32:59.645960Z",
     "iopub.status.idle": "2025-09-12T14:32:59.655395Z",
     "shell.execute_reply": "2025-09-12T14:32:59.654762Z",
     "shell.execute_reply.started": "2025-09-12T14:32:59.646420Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n"
     ]
    }
   ],
   "source": [
    "word = tokenizer.index_word[1]\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### so we split data in a way so that the model trains on 7 words and predict the 8th and a step by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:32:59.656266Z",
     "iopub.status.busy": "2025-09-12T14:32:59.656062Z",
     "iopub.status.idle": "2025-09-12T14:32:59.672016Z",
     "shell.execute_reply": "2025-09-12T14:32:59.671264Z",
     "shell.execute_reply.started": "2025-09-12T14:32:59.656251Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 901, 902, 228, 195, 402, 8] -> 560\n",
      "[901, 902, 228, 195, 402, 8, 560] -> 125\n",
      "[902, 228, 195, 402, 8, 560, 125] -> 403\n",
      "[228, 195, 402, 8, 560, 125, 403] -> 126\n",
      "[195, 402, 8, 560, 125, 403, 126] -> 270\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "seq_length = 7\n",
    "for i in range(seq_length, len(sequence)):\n",
    "    if i < seq_length + 5:  # only show the first 5 examples\n",
    "        print(sequence[i-seq_length:i], \"->\", sequence[i])\n",
    "    x.append(sequence[i-seq_length:i])\n",
    "    y.append(sequence[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert the x (7 words) and y (the 8th word) to numpy array to match the inout expected by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:32:59.673061Z",
     "iopub.status.busy": "2025-09-12T14:32:59.672848Z",
     "iopub.status.idle": "2025-09-12T14:32:59.690190Z",
     "shell.execute_reply": "2025-09-12T14:32:59.689651Z",
     "shell.execute_reply.started": "2025-09-12T14:32:59.673045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print the shape of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:32:59.691041Z",
     "iopub.status.busy": "2025-09-12T14:32:59.690881Z",
     "iopub.status.idle": "2025-09-12T14:32:59.701990Z",
     "shell.execute_reply": "2025-09-12T14:32:59.701363Z",
     "shell.execute_reply.started": "2025-09-12T14:32:59.691028Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10366, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:32:59.704527Z",
     "iopub.status.busy": "2025-09-12T14:32:59.704302Z",
     "iopub.status.idle": "2025-09-12T14:32:59.715755Z",
     "shell.execute_reply": "2025-09-12T14:32:59.715075Z",
     "shell.execute_reply.started": "2025-09-12T14:32:59.704513Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10366"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use train test splpit mehtos for better evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:32:59.716522Z",
     "iopub.status.busy": "2025-09-12T14:32:59.716353Z",
     "iopub.status.idle": "2025-09-12T14:32:59.811168Z",
     "shell.execute_reply": "2025-09-12T14:32:59.810588Z",
     "shell.execute_reply.started": "2025-09-12T14:32:59.716508Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tain, x_test , y_train , y_test = train_test_split(x , y , test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import the layers to build the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:32:59.812120Z",
     "iopub.status.busy": "2025-09-12T14:32:59.811907Z",
     "iopub.status.idle": "2025-09-12T14:32:59.820585Z",
     "shell.execute_reply": "2025-09-12T14:32:59.819823Z",
     "shell.execute_reply.started": "2025-09-12T14:32:59.812103Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model , Sequential \n",
    "from tensorflow.keras.layers import Dense , Embedding , LSTM , add , Dropout\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so each word in the sequence is embedded for semantic meaning then fed into the LSTM the sequence word by word till the last word where we have an output vector of the sentence then a dropout by 0.3 to reduce overfitting and feed the result to a dense layer then do a probability distribtion of all the words the one with the highest probability is chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compile , train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:32:59.821700Z",
     "iopub.status.busy": "2025-09-12T14:32:59.821403Z",
     "iopub.status.idle": "2025-09-12T14:33:30.203730Z",
     "shell.execute_reply": "2025-09-12T14:33:30.203181Z",
     "shell.execute_reply.started": "2025-09-12T14:32:59.821667Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "I0000 00:00:1757687580.777491      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1757687580.778303      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1757687585.516376     101 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.0214 - loss: 7.3314\n",
      "Epoch 2/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0254 - loss: 6.3828\n",
      "Epoch 3/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0248 - loss: 6.2318\n",
      "Epoch 4/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0271 - loss: 6.1483\n",
      "Epoch 5/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0312 - loss: 6.0912\n",
      "Epoch 6/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0390 - loss: 6.0255\n",
      "Epoch 7/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0432 - loss: 5.9215\n",
      "Epoch 8/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0478 - loss: 5.8134\n",
      "Epoch 9/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0543 - loss: 5.7124\n",
      "Epoch 10/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0598 - loss: 5.5458\n",
      "Epoch 11/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0684 - loss: 5.3683\n",
      "Epoch 12/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0834 - loss: 5.1762\n",
      "Epoch 13/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0979 - loss: 4.9873\n",
      "Epoch 14/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1052 - loss: 4.7991\n",
      "Epoch 15/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1269 - loss: 4.5646\n",
      "Epoch 16/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1449 - loss: 4.3914\n",
      "Epoch 17/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1668 - loss: 4.1640\n",
      "Epoch 18/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2030 - loss: 3.9559\n",
      "Epoch 19/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2471 - loss: 3.6937\n",
      "Epoch 20/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2961 - loss: 3.5072\n",
      "Epoch 21/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3509 - loss: 3.2529\n",
      "Epoch 22/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3942 - loss: 3.0036\n",
      "Epoch 23/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4448 - loss: 2.7957\n",
      "Epoch 24/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4914 - loss: 2.5727\n",
      "Epoch 25/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5351 - loss: 2.3739\n",
      "Epoch 26/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5691 - loss: 2.1937\n",
      "Epoch 27/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6046 - loss: 2.0094\n",
      "Epoch 28/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6467 - loss: 1.8442\n",
      "Epoch 29/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6861 - loss: 1.6581\n",
      "Epoch 30/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7217 - loss: 1.4941\n",
      "Epoch 31/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7443 - loss: 1.3713\n",
      "Epoch 32/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7777 - loss: 1.2333\n",
      "Epoch 33/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8001 - loss: 1.1261\n",
      "Epoch 34/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8322 - loss: 1.0088\n",
      "Epoch 35/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8509 - loss: 0.8971\n",
      "Epoch 36/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8646 - loss: 0.8197\n",
      "Epoch 37/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8912 - loss: 0.7151\n",
      "Epoch 38/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8958 - loss: 0.6650\n",
      "Epoch 39/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9165 - loss: 0.5863\n",
      "Epoch 40/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9263 - loss: 0.5356\n",
      "Epoch 41/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9427 - loss: 0.4672\n",
      "Epoch 42/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9474 - loss: 0.4275\n",
      "Epoch 43/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9596 - loss: 0.3778\n",
      "Epoch 44/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9631 - loss: 0.3374\n",
      "Epoch 45/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9656 - loss: 0.3150\n",
      "Epoch 46/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9683 - loss: 0.2835\n",
      "Epoch 47/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9761 - loss: 0.2552\n",
      "Epoch 48/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9796 - loss: 0.2275\n",
      "Epoch 49/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9837 - loss: 0.2078\n",
      "Epoch 50/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9855 - loss: 0.1846\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(words_vocab, 256, input_length=seq_length))               # optional\n",
    "model.add(LSTM(256, return_sequences=False))\n",
    "model.add(Dropout(0.3))                 # recommended\n",
    "model.add(Dense(words_vocab, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "model.compile(\n",
    "   loss=\"sparse_categorical_crossentropy\",  # handles integer labels\n",
    "   optimizer=\"adam\",\n",
    "   metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.fit(x, y, epochs=50, batch_size=128, verbose=1)\n",
    "\n",
    "model.save(\"lstm_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we got a perfect accuracy 0.9855 and a perfect loss also 0.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evluating the model on the test data and it's perfect there's no overfitting accuracy 0.99 and loss 0.06 which is perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:33:30.204723Z",
     "iopub.status.busy": "2025-09-12T14:33:30.204498Z",
     "iopub.status.idle": "2025-09-12T14:33:30.814629Z",
     "shell.execute_reply": "2025-09-12T14:33:30.814071Z",
     "shell.execute_reply.started": "2025-09-12T14:33:30.204706Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9991 - loss: 0.0691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07384383678436279, 0.9985535144805908]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test , y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a function to generate words from a sentence that we feed to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:33:30.815486Z",
     "iopub.status.busy": "2025-09-12T14:33:30.815303Z",
     "iopub.status.idle": "2025-09-12T14:33:30.820222Z",
     "shell.execute_reply": "2025-09-12T14:33:30.819497Z",
     "shell.execute_reply.started": "2025-09-12T14:33:30.815472Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_poem(seq_length , input_text):\n",
    "    result = input_text\n",
    "    for _ in range(seq_length):\n",
    "        word_index = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        token_list = word_index[-seq_length:]\n",
    "        token_list = np.array(token_list).reshape(1 ,-1)\n",
    "        predicted_probs = model.predict(token_list , verbose=0)\n",
    "        predicted_index = np.argmax(predicted_probs , axis=-1)[0]\n",
    "        predicted_word = tokenizer.index_word.get(predicted_index, \"\")\n",
    "        input_text += \" \" + predicted_word\n",
    "        result += \" \" + predicted_word\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a test generate 100 words from this sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:33:30.821169Z",
     "iopub.status.busy": "2025-09-12T14:33:30.820948Z",
     "iopub.status.idle": "2025-09-12T14:33:39.156402Z",
     "shell.execute_reply": "2025-09-12T14:33:39.155594Z",
     "shell.execute_reply.started": "2025-09-12T14:33:30.821153Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From fairest creatures we desire increase things besmeared or merit alone i time i cold alone look look despite the sight in my sight thou what thou mayst thou me look look look look look look look look look look look look look look look look look look look look look look 'tis such my heart the spirit that thou such my heart mine own heart mine own desert and by thy pleasure mine eye the spirit in mine own love's silent such mine eye and thou such a living day a living hue a tattered sight of me the heart of thy love and i\n"
     ]
    }
   ],
   "source": [
    "print(generate_poem(100 ,\"From fairest creatures we desire increase\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another test with 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:33:39.157549Z",
     "iopub.status.busy": "2025-09-12T14:33:39.157274Z",
     "iopub.status.idle": "2025-09-12T14:33:39.923367Z",
     "shell.execute_reply": "2025-09-12T14:33:39.922540Z",
     "shell.execute_reply.started": "2025-09-12T14:33:39.157515Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Those hours that with gentle work did frame no frame no fair of many is it is so\n"
     ]
    }
   ],
   "source": [
    "print(generate_poem(10 , \"Those hours that with gentle work did frame\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:35:36.020766Z",
     "iopub.status.busy": "2025-09-12T14:35:36.020177Z",
     "iopub.status.idle": "2025-09-12T14:35:36.777753Z",
     "shell.execute_reply": "2025-09-12T14:35:36.777102Z",
     "shell.execute_reply.started": "2025-09-12T14:35:36.020742Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which happies those that pay the willing loan loan loan loan that's for thy self to honour\n"
     ]
    }
   ],
   "source": [
    "print(generate_poem(10 , \"Which happies those that pay the willing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T14:38:52.340779Z",
     "iopub.status.busy": "2025-09-12T14:38:52.340486Z",
     "iopub.status.idle": "2025-09-12T14:38:53.113141Z",
     "shell.execute_reply": "2025-09-12T14:38:53.112402Z",
     "shell.execute_reply.started": "2025-09-12T14:38:52.340758Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From fairest creatures we desire increase,\n",
      "That thereby beauty's rose might never die,\n",
      "But as the riper should by time decease,\n",
      "His tender heir might bear his memory:\n",
      "But thou contracted to thine own bright eyes,\n",
      "Feed thy light's flame with self-substantial fuel,\n",
      "Making a famine where abundance lies,\n",
      "Thy self thy foe, to thy sweet self too cruel: thy sweet use thy name and found thy ripe thoughts\n"
     ]
    }
   ],
   "source": [
    "print(generate_poem(10, \"\"\"From fairest creatures we desire increase,\n",
    "That thereby beauty's rose might never die,\n",
    "But as the riper should by time decease,\n",
    "His tender heir might bear his memory:\n",
    "But thou contracted to thine own bright eyes,\n",
    "Feed thy light's flame with self-substantial fuel,\n",
    "Making a famine where abundance lies,\n",
    "Thy self thy foe, to thy sweet self too cruel:\"\"\"))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5940695,
     "sourceId": 9711935,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
